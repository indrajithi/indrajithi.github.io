[{"content":" Recursive queries are typically used to deal with hierarchical or tree-structured data. A common example is when you have a manager \u0026gt; employee relation in a table and you have to construct the organization tree under a manager or find all N level managers of an employee.   Strictly speaking, this process is iteration not recursion, but RECURSIVE is the terminology chosen by the SQL standards committee.\n PostgreSQL CTE: Common Table Expression From the docs:\n WITH provides a way to write auxiliary statements for use in a larger query. These statements, which are often referred to as Common Table Expressions or CTEs, can be thought of as defining temporary tables that exist just for one query.\n  A common table expression is a temporary result set which you can reference within another SQL statement.\n  A useful property of WITH queries is that they are evaluated only once per execution of the parent query, even if they are referred to more than once by the parent query or sibling WITH queries.\n  Thus, expensive calculations that are needed in multiple places can be placed within a WITH query to avoid redundant work.\n  Another possible application is to prevent unwanted multiple evaluations of functions with side-effects\n Syntax:\nWITH cte_name (column_list) AS ( CTE_query_definition ) statement; Example:\nWITH regional_sales AS ( SELECT region, SUM(amount) AS total_sales FROM orders GROUP BY region ), top_regions AS ( SELECT region FROM regional_sales WHERE total_sales \u0026gt; (SELECT SUM(total_sales)/10 FROM regional_sales) ) SELECT region, product, SUM(quantity) AS product_units, SUM(amount) AS product_sales FROM orders WHERE region IN (SELECT region FROM top_regions) GROUP BY region, product;   The above query has two auxiliary statements named regional_sales and top_regions. The output of regional_sales is used in top_regions and the output of top_region is used in the primary SELECT query. We can write the above query without CTE, but it\u0026rsquo;d require us to write two levels of nested sub-SELECTs It is easier to follow this way and the auxiliary statements are executed only once although they are referenced multiple times.  RECURSIVE WITH  Using RECURSIVE, a WITH query can refer to its own outupt.  Lets took in to an example which counts from 1 to 20:\nWITH RECURSIVE cte AS (SELECT 1 AS n -- anchor member  UNION SELECT n + 1 -- recursive member  FROM cte WHERE n \u0026lt; 20 -- terminator  ) SELECT n FROM cte;  n ---- 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  Let\u0026rsquo;s create some hierarchial data to play with CREATE TABLE employees ( id serial PRIMARY KEY, name VARCHAR NOT NULL, manager_id INT ); INSERT INTO employees (id, name, manager_id) VALUES (1, \u0026#39;Adam Smith\u0026#39;, NULL), (2, \u0026#39;John Nash\u0026#39;, 1), (3, \u0026#39;Mary Jones\u0026#39;, 1), (4, \u0026#39;Peter Gregery\u0026#39;, 2), (5, \u0026#39;Sam Joey\u0026#39;, 3), (6, \u0026#39;Tim Lee\u0026#39;, 4), (7, \u0026#39;Mohan Lal\u0026#39;, 5), (8, \u0026#39;Will Smith\u0026#39;, 6), (10, \u0026#39;Mohan Rathod\u0026#39;, 8); Now lets find all the people working under John Nash. WITH RECURSIVE tree as ( SELECT id, name, manager_id FROM employees WHERE id = 2 UNION SELECT e.id, e.name, e.manager_id FROM employees as e JOIN tree t ON t.id = e.manager_id ) SELECT id, name, manager_id FROM tree; SELECT id, name, manager_id from tree; id | name | manager_id ----+---------------+------------ 2 | John Nash | 1 4 | Peter Gregery | 2 6 | Tim Lee | 4 8 | Will Smith | 6 10 | Mohan Rathod | 8 (5 rows)  The first part of the query Anchor Member selects the Manager participant John Nash identified by id = 2 Then we join that result with employees table on t.id = e.manager_id to find all the employees having John Nash as their manager This is repeated until all the results are returned  How about all the N level managers of Mohan Rathod?  We just need to reverse the condition on the JOIN  WITH RECURSIVE tree as ( SELECT id, name, manager_id FROM employees where id = 8 UNION SELECT e.id, e.name, e.manager_id FROM employees as e JOIN tree t ON t.manager_id = e.id ) SELECT id, name, manager_id FROM tree;  id | name | manager_id ----+---------------+------------ 8 | Will Smith | 6 6 | Tim Lee | 4 4 | Peter Gregery | 2 2 | John Nash | 1 1 | Adam Smith | (5 rows) How to prevent infinite loop when are cycles in the data The recursive query can run infinitely when there are cycles in the data.\n Lets create an infinite loop\n In the above data if we update the manager_id with id. The manager for the employee will be that employee itself.\nUPDATE employees SET manager_id = 8 where id = 8;\nNow when we try to find all the people working under employees.id = 8 it should run infinitely?\nWITH RECURSIVE tree as ( SELECT id, name, manager_id FROM employees where id = 8 UNION ALL SELECT e.id, e.name, e.manager_id FROM employees as e JOIN tree t ON t.id = e.manager_id ) SELECT id, name, manager_id FROM tree;  A helpful trick for testing queries when you are not certain if they might loop is to place a LIMIT in the parent query like this SELECT id, name, manager_id FROM tree limit 10; (Not recommended in Production)\n  Sometimes, using UNION instead of UNION ALL can remove the infinite loop by discarding rows that duplicate previous output rows. However, often a cycle does not involve output rows that are completely duplicate:\n Take this for example:\nWITH RECURSIVE tree as ( SELECT id, name, manager_id, 1 as depth FROM employees WHERE id = 2 UNION SELECT e.id, e.name, e.manager_id, t.depth + 1 FROM employees as e JOIN tree t ON t.id = e.manager_id ) SELECT id, name, manager_id FROM tree; SELECT id, name, manager_id, depth from tree; id | name | manager_id | depth ----+---------------+------------+------- 2 | John Nash | 1 | 1 4 | Peter Gregery | 2 | 2 6 | Tim Lee | 4 | 3 (3 rows)  When we run this query for employees.id = 8 where there is a self referencing cycle, UNION will not solve our problem since each rows returned had distinct depth value   You can try this (runs infinitely):\n WITH RECURSIVE tree as ( SELECT id, name, manager_id, 1 as depth FROM employees WHERE id = 8 UNION SELECT e.id, e.name, e.manager_id, t.depth + 1 FROM employees as e JOIN tree t ON t.id = e.manager_id ) SELECT id, name, manager_id FROM tree;  To solve this type of problem, we can modify the query like this:\n WITH RECURSIVE tree as ( SELECT id, name, manager_id, 1 as depth, ARRAY[id] as path, false as cycle FROM employees WHERE id = 8 UNION SELECT e.id, e.name, e.manager_id, t.depth + 1, t.path || e.id, e.id = ANY(t.path) FROM employees as e JOIN tree t ON t.id = e.manager_id WHERE NOT t.cycle ) SELECT id, name, manager_id, path, cycle FROM tree;  id | name | manager_id | path | cycle ----+--------------+------------+--------+------- 8 | Will Smith | 8 | {8} | f 10 | Mohan Rathod | 8 | {8,10} | f 8 | Will Smith | 8 | {8,8} | t (3 rows)  When there are multiple field to be checked to recognize a cycle, use an array of rows.\n WITH RECURSIVE tree as ( SELECT id, name, manager_id, 1 as depth, ARRAY[ROW(id, manager_id)] as path, false as cycle FROM employees WHERE id = 8 UNION ALL SELECT e.id, e.name, e.manager_id, t.depth + 1, t.path || ROW(e.id, e.manager_id), ROW(e.id, e.manager_id) = ANY(t.path) FROM employees as e JOIN tree t ON t.id = e.manager_id WHERE NOT t.cycle ) SELECT id, name, manager_id, path, cycle FROM tree;  id | name | manager_id | path | cycle ----+--------------+------------+--------------------+------- 8 | Will Smith | 8 | {\u0026quot;(8,8)\u0026quot;} | f 10 | Mohan Rathod | 8 | {\u0026quot;(8,8)\u0026quot;,\u0026quot;(10,8)\u0026quot;} | f 8 | Will Smith | 8 | {\u0026quot;(8,8)\u0026quot;,\u0026quot;(8,8)\u0026quot;} | t (3 rows) Reference:\n https://www.postgresqltutorial.com/postgresql-recursive-query/ https://www.postgresql.org/docs/9.1/queries-with.html https://hakibenita.com/be-careful-with-cte-in-postgre-sql  ","permalink":"https://indrajith.me/posts/recursive-queries-in-postgresql-for-hierarchial-data/","summary":"Recursive queries are typically used to deal with hierarchical or tree-structured data. A common example is when you have a manager \u0026gt; employee relation in a table and you have to construct the organization tree under a manager or find all N level managers of an employee.   Strictly speaking, this process is iteration not recursion, but RECURSIVE is the terminology chosen by the SQL standards committee.\n PostgreSQL CTE: Common Table Expression From the docs:","title":"Recursive Queries in PostgreSQL for Hierarchical Data"},{"content":" Happiness is the state between one desire being fulfilled and new desire forming. Suffering is the state between craving a change in state and achieving it.\n The Compounding Power of Atomic Habits  The effects of small habits compound over time. If you can get 1% better each day, by the end of the year you\u0026rsquo;d have become 37 times better. Conversely with bad habits, if you had have become 1% worse every day for one year you\u0026rsquo;ll decline nearly down to zero.  The task of breaking bad habits is like uprooting an awk from within us. The task of creating good habits is like cultivating a flower plant one day at a time.\n A bamboo takes 5 years to establish an extensive network of roots underneath before it starts to grow from the ground up, after that it only take it 6 weeks to reach 90 feet (~27 Meeters) into the air.\n This is the case for bad habits as well. If you are having a chronic back pain, chances are it have not developed overnight as it is apparent. It may be the result your sitting habits and all yoga or workout you did\u0026rsquo;nt to do everyday that have created a compounding effect of bad habits which have caused that chronic pain.\nPlateau of Latent Potential  The time period in which you see no apparent effect of your action.\n When there is no short term reward for the tiny directions or actions you take, people will often get demotivated. When you fail to see a tangible result, you decide to stop. In order to make meaningful difference, habits needs to persist long enough to break through this plateau — what James Clear calls the Plateau of Latent Potential\nIf you find yourself struggling to build good habits or break bad ones, you have not yet crossed the Plateau of Latent Potential\n When nothing seems to work, the author watches a stonecutter hammering a stone to break it. He might have taken 25 hits to finally break it. We know that it is not just the 25th hit that have broken the stone, but all that have gone before.\n Do not set Goals. What!! Why?  True long-term strategy is goal-less thinking\n Goals are good in setting a direction, but systems are best for making progress. We often waste awful lot of time setting goals. And very little time designing a system that can help us achieve those goals.\nProblem #1: Winners and losers have the same goals  If successful and unsuccessful people share the same goals, then the goal cannot be what differentiates the winners from the losers.\n Problem #2: Achieving a goal is only a momentary change  Imagine you have a messy room and the goal is to clean the room. And one day you summon the energy to clean it, then you\u0026rsquo;ll have a clean room - for now. But if you maintain the same sloppy habits that led to a messy room in the first place, soon you\u0026rsquo;ll be looking at a pile of clutter and hopping for another burst of motivation.\n You are left chasing the sane outcome because you never changed the system behind it. You treat the symptom without addressing the cause. The cause is your unorganized behavior. What you should be addressing is to create a system to be organized.\nProblem #3: Goals restrict your happiness  The implicit assumption behind a goal is that: Once I achieve my goal, then I will be happy. The problem with this goal first mentality is that you are continually putting happiness off until the goal is achieved. Either you achieve your goal and are successful or you fail and you are disappointed.\n When you fall in love with the process rather than the result, you can be happy anytime and every time your system is running.\nProblem #4: Goals are at odds with long-term progress You find people often themselves reverting to their old habits after they accomplish a goal. Take the above example of a clean room\n The purpose of setting a goal is to win the game. The purpose of building system is to continue playing the game. True long-term thinking is goal-less thinking. It is not about a single accomplishment, but a cycle of endless refinement and continuous improvements.\n Own your behavior so that you can change it  Imagine you are a smoker and want to quit smoking. When someone offered a cigarette you refused by saying either of the below two response.\n  I am trying to quit I am not a smoker  The person giving the first response actually believes that he is a smoker. And have created that belief have come from the identity he have created.\nIn the second response the person actually does not identifies with someone who smokes. It is part of their former self and he is not that person anymore.\n There are three layers of behavior change\n  A change in your outcomes (loosing weight, publishing a book..) A change in your process (developing a meditation routine) A change in your identity (believes, biases, and wold view)  When you write each day, you embody the identity of a creative person. Whe you make your bed everyday you embody the identity of an organized person When you do meditation/yoga everyday you embody the identity of a spiritual person\n The more you repeat a behavior. The more you re-enforce the identity associated with that behavior The most effective way the change your habits is to focus not on what you want to achieve, but on who you want to become\n How to build habits  All habits proceed through four stages in the same order:\n  Cue: anything that triggers a craving. (seeing someone smoke, an advertisement, smell of food) Craving: anything that creates an ardent need for a response. (craving to smoke, craving to eat) Response: any action that produces a reward Reward: anything that is very satisfying  How to creating new habits  Cue: Make it obvious Craving: Make it attractive Response: Make it easy Reward: Make it satisfying  How to breaking bad habits  Cue: Make it invisible Craving: Make it unattractive Response: Make it difficult Reward: Make it unsatisfying  \u0026ndash;\u0026gt;\nCredits Atomic Habits by James Clear\n","permalink":"https://indrajith.me/posts/on-habits-and-behavior-change/","summary":"Happiness is the state between one desire being fulfilled and new desire forming. Suffering is the state between craving a change in state and achieving it.\n The Compounding Power of Atomic Habits  The effects of small habits compound over time. If you can get 1% better each day, by the end of the year you\u0026rsquo;d have become 37 times better. Conversely with bad habits, if you had have become 1% worse every day for one year you\u0026rsquo;ll decline nearly down to zero.","title":"On habits and behavior change"},{"content":"In this post, I’ll share the step by step process we used at AirCTO to create computerized adaptive tests.\nAt AirCTO we use adaptive tests to measure the ability of the candidate in different domains (Python, Front end development, Docker etc,.).\nBack to basics: What is computerized adaptive test?  Computerized adaptive testing (CAT)) is a form of computer-based test that adapts to the examinee\u0026rsquo;s ability level. For this reason, it has also been called tailored testing. In other words, it is a form of computer-administered test in which the next item or set of items selected to be administered depends on the correctness of the test taker\u0026rsquo;s responses to the most recent items administered.\n Why we use the adaptive test? Adaptive tests are designed to challenge candidates. High-achieving candidates who take a test that is designed around an average are not challenged by questions below their individual achievement abilities.\nLikewise, if lower-achieving candidates served by questions that are far above their current abilities they are left guessing the answers instead of applying what they already know.\nAdaptive testing addresses these issues by adjusting the questions to the individual proficiency of the candidate. High-achieving candidates are challenged by more difficult questions, while candidates who are slightly below the average are not overwhelmed but rather encouraged to continue moving forward by answering questions at or slightly above their current achievement level.\n  This motivates candidates to reach slightly beyond their comfort zone no matter where that zone might be-and prevents them from getting discouraged.\n  The adaptive test is based on a statistical model defined by the Item Response Theory.\n  We measure the ability of the candidate and the question difficulty on the same scale.\n   True Score/ True Ability: True score is the score an examinee would receive on a perfectly reliable test. Since all tests contain error, true scores are a theoretical concept; in an actual testing program, we will never know an individual’s true score.\n  We can however, compute an estimate of an examinee’s true score and we can estimate the amount of error in that estimate. True ability is denoted as θ; the true score for examinee j is denoted θ-j.\n Item Response Theory (IRT): Item Response Theory (IRT) is a statistical framework in which examinees can be described by a set of one or more ability scores that are predictive, through mathematical models, linking actual performance on test items, item statistics, and examinee abilities.\nIRT states that the probability of an examinee correctly answering the question is a function of the candidates true ability (θ) and the difficulty of the question (bi).\nSource\nUnder the 3 parameter IRT model, the probability of a correct response to a given item is a function of an examinee’s true ability and three item parameters:\nItem parameters: The parameters are calculated based on prior administrations of the items to a sample population. This is called pilot-test. It is possible to calibrate item parameters from the responses on a test.\nInitial item parameters are selected by modeling IRT parameters with responses from a sample population.\n1. ai : Item Discrimination Parameter This parameter shows how well an item (question) discriminates individuals who answer the item correctly and those who don’t.\nAn item with a high value tends to be answered correctly by all individuals whose θ is above the items' difficulty level and wrongly by all the others.\n2. bi : Item Difficulty Parameter b represents an item’s difficulty parameter. This parameter is measured on the same scale as θ. It shows at which point of the proficiency scale the item is more is informative, that is, where it discriminates the individuals who agree and those who disagree with the item.\nSince b and θ are measured in the same scale, b follows the same distributions as θ.\nFor a CAT, it is good for an item bank to have as many items as possible in all difficulty levels, so that the CAT may select the best item for each individual in all ability levels.\n3. ci : Guessing parameter c represents an item’s pseudo-guessing parameter. This parameter denotes what is the probability of individuals with low proficiency values to answer the item correctly. Since c is a probability, 0\u0026lt; c ≤1, the lower the value of this parameter, the better the item is considered.\nLet’s assume that you are given a question with four options and you do know the correct answer. If you select an answer randomly there is a 0.25 chance of success. This is the guessing parameter.\nSome times when you know the answer partially, one question might feel more probable than others. In this case the guessing parameter will be the highest probable one.\n4. di : Upper asymptote  An asymptote of a curve is a line such that the distance between the curve and the line approaches zero as one or both of the x or y coordinates tends to infinity.\n d represents an item’s upper asymptote. This parameter denotes what is the probability of individuals with high proficiency values to still answer the item incorrectly.\nSince d is a probability, 0\u0026lt; d ≤1, the higher the value of this parameter, the better the item is considered.\nIn our implementation, we have used three-parameter model (3PL) with parameters a, b, c and d is a constant numpy.ones((n)).\n Each item has a different set of these three parameters. These parameters are usually calculated based on prior administrations of the item.\n  You can simulate these parameters using the following probability distributions:\n discrimination: N(1.2,0.25) difficulty: N(0,1) pseudo_guessing: N(0.25,0.02) upper asymptote: N(0.93,0.02)  \n Once we have enough samples these item parameters can be re-calibrated for better performance. θ is the examinee’s true ability, θ^ is the estimated ability.\n The 3PL IRT model states that probability of a correct response to an item i for an examinee is a function of the three item parameters and examinee j’s true ability θj.\nUnder IRT, the probability of an examinee with a given θ^ value (estimated ability), to answer item i correctly, given the item parameters, is given by:\nWith IRT, maximum information can be quantified as the standardized slope of ( θ) at θ^ . In other words\nThe CAT (Computerized Adaptive Test) algorithm: is usually an iterative process with the following steps:\n  Ability Estimation: All the items that have not yet been administered are evaluated to determine which will be the best one to administer as next-item, given the currently estimated ability level.\n  Item Selection: The “best” next item is administered and the examinee responds.\n  A new ability estimate is computed based on the responses to all of the administered items.(Log likelyhood, DifferentialEvolutionEstimator, HillClimbingEstimator)\n  Steps 1 through 3 are repeated until a stopping criterion is met.\n  Ability Initialisation: (only first time) We can initialize Randomly or on Fixed Point (-4, 4).\nAbility estimation: (except first time): A new ability estimate is computed based on the responses to all of the administered items. There are two main types of ways of estimating θ^: and these are the Bayesian methods and maximum-likelihood ones.\n Maximum-likelihood methods choose the θ^ value that maximizes the log likelihood of an examinee having a certain response vector, given the corresponding item parameters.\n  Bayesian methods used a priori information (usually assuming proficiency and parameter distributions) to make new estimations. The knowledge of new estimations is then used to make new assumptions about the parameter distributions, refining future estimations. [1]\n Item Selection: We used an implementation of the random sequence selector in which, at every step of the test, an item is randomly chosen from the n most informative items in the item bank, n being a predefined value.\nStopping rule: The stopping criterion could be time, the number of items administered, change in ability estimate, content coverage, a precision indicator such as the standard error.\n We use a combination of : time, ability estimate and precision.\n In other words:\n With IRT, maximum information can be quantified as the standardised slope of Pi( theta ) at theta_hat.\n  Cut Score Determination: While there are many methods for setting the cut score, there are a few that easily incorporate the information provided by IRT methods.\n  That is, many methods such as The θ values obtained via IRT provide a unique opportunity to get a more reliable estimate of respondent scores, and you are wise to look for a cut score method that can incorporate that info and typically use observed scores for determining the cut score, and inherently ignore the measurement error involved in that. in IRT: Pass/Fail\n The adaptive test will give the ability of the user in the interval -4 to +4 (low, high). We can either use this scale or convert it to a different scale: Transform your data using a desired mean and standard deviation.\nReference  A visual guide to item response theory Interactive, Computer Adaptive Testing Tutorial. Cut Score Determination Computerized Adaptive Testing Simulator  ","permalink":"https://indrajith.me/posts/computerized-adaptive-test/","summary":"In this post, I’ll share the step by step process we used at AirCTO to create computerized adaptive tests.\nAt AirCTO we use adaptive tests to measure the ability of the candidate in different domains (Python, Front end development, Docker etc,.).\nBack to basics: What is computerized adaptive test?  Computerized adaptive testing (CAT)) is a form of computer-based test that adapts to the examinee\u0026rsquo;s ability level. For this reason, it has also been called tailored testing.","title":"How to develop a Computerized Adaptive Test (CAT)"},{"content":"Music Genre Classifier Demo Music is categorized into subjective categories called genres. With the growth of the internet and multimedia, systems applications that deal with the musical databases gained importance and demand for Music Information Retrieval (MIR) applications increased.\nMusical genres have no strict definitions and boundaries as they arise through a complex interaction between the public, marketing, historical, and cultural factors. We are going to create an application that accepts a music file and classify it in to genres.\nRequirements  Django (1.11) Numpy (1.12.1) Scikit-Learn (0.18.1) Scipy (0.19.0) Python-Speech-Features (0.5) Pydub (0.18.0)  Installation  git clone https://github.com/indrajithi/mgc-django.git pip install -r requirements.txt python manage.py migrate python manage.py runserver App will run on localhost:8000  Music Genre Classifier Our app is written in Python using Django framework. We are using a trained Poly Kernel SVM for finding the genre.\nOur web application uses the package mysvm which we developed to extract features and to find the genre.\nWe have developed a python package called mysvm for extracting features and classifying music. This package is added to our App. On receiving a request for genre label, we convert the file to .wav if it is in other format. Then the features are extracted from the audio file using mysvm.feature.extract (filename).\nPrototype We need to find the best classification algorithm that can be used in our App. Matlab is ideal to implement machine learning algorithms in minimum lines of code. Before making the App in python we made a prototype in Matlab.\nFeature Extraction We chose to extract MFCC from the audio files as the feature. For finding MFCC in Matlab, we have used HTK MFCC MATLAB toolkit. The output will be a matrix of 13n dimensional vector. Where n depends on the total duration of the audio. 13(100*sec).\nWhile feature extraction we were getting ‘nan’(not a number) and infinity in the output. This is usually caused be a division by zero or a very small number closed to 0 resulting in infinity/nan in the output. This could be a regular result or some algorithm or implementation error in the MFCC toolkit. To overcome this situation, we have set nan or infinity entries in the feature array to 0.\nPrincipal Component Analysis. Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components (or sometimes, principal modes of variation) [7]. After doing a PCA on the data we got 90% variance and should reduce the feature dimension.\n[input2, eigvec, eigvalue] = pca (ds. input); cumvar = cumsum(eigvalue); //cumulative sum n(n+1)/2 cumvarpercent = cumvar/cumvar(end)*100; Dimensionality reduction Various researchers take statistics such as mean variance IQR, etc., to reduce the feature dimension. Some researchers model it using multivariate regression and some fit it to a Gaussian mixture model. Here we are taking the mean and upper diagonal variance of 13*n MFCC coefficients. The result is a feature vector of size 104.\n%Reducing Feature Dimeansion mf = mean(mm,2); %row mean cf = cov(mm\u0026#39;); % covariance ff = mf; for i=0:(size(mm,1)-1) ff = [ff;diag(cf,i)]; %use diagonals  end t(:,end+1) = ff(:,1); Classification K-nearest neighbors (KNN) Principle is that the data instance of the same class should be closer in the feature space. For a given data point x of unknown class, we can compute the distance between x and all the data points in the training data and assign the class determined by k nearest points of x.\nSuppose we are given training dataset of n points. {(x1,y1),(x2,y2)…(xn,yn)} Where (xi,yi) represents data pair i. xi- feature vector yi- target class For a new data point x the most likely class is determined by finding the distance from all training data points (Euclidian distance). The output class will be the class which k nearest neighbors belongs to. K is a predefined integer (k=1, k=2, k=3.)\nLogistic Regression Logistic Regression is one of the widely used classification algorithm. This algorithm is used in medical as well as business fields for analytics and classification. This model has the hypothesis function 0 ≤h (x) ≤ 1. Where hθ(x) = 11 + e-θTx called as Sigmoid or Logistic Function. For binary class classification y ∈{0, 1}. The output of this classifier will be a probability of the given input belonging to class 1. Ifhθ(x)outputs 0.7 it means that the given input has 70% chance of belonging to class 1. Since we have 10 genre classes y ∈{0, 1 .. 9}we used one-vs-all method for classification.\nPython package mysvm We developed a python package called mysvm which contains three modules: features, svm, acc. These are used by the web application in feature extraction and finding genre. This package also contains many other functions to do complicated feature extraction and classification.\n├── acc.py ├── data │ ├── classifier_10class.pkl │ ├── classifier_10class_prob.pkl │ ├── cmpr.pkl │ └── Xall.npy ├── feature.py ├── __init__.py ├── svm.py feature This module is used to extract MFCC features from a given file. It contains the following functions.\n extract (file): Extract features from a given file. Files in other formats are converted to .wav format. Returns numpy array. extract_all (audio_dir): Extracts features from all files in a directory. extract_ratio (train_ratio, test_ratio, audio_dir) : Extract features from all files in a directory in a ratio. Returns two numpy arrays. geny(n): Generates Y values for n classes. Returns numpy array. gen_suby(sub, n): Generates Y values for a subset of classes. Returns numpy array. gen_labels( ): Returns a list of all genre labels. flattern( x) : Flatterns a numpy array.  svm A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. This module contains various functions for classification using support vector machines.\n  poly(X,Y): Trains a poly kernel SVM by fitting X, Y dataset. Returns a trained poly kernel SVM classifier.\n  fit ( training_percentage, fold): Randomly choose songs from the dataset, and train the classifier. Accepts parameter: train_percentage, fold; Returns trained classifier.\n  getprob (filename): Find the probabilities for a song belongs to each genre. Returns a dictionary mapping genre names to probability and a list of top 3 genres which is having probability of more than 0.15.\n  random_cross_validation (train_percentage,fold): Randomly cross validate with training percentage and fold. Accepts parameter: train_percentage, fold;\n  findsubclass (class_count): Returns all possible ways we can combine the classes. Accepts an integer as class count. Returns numpy array of all possible combination.\n  gen_sub_data (class_l): Generate a subset of the dataset for the given list of classes. Returns numpy array.\n  fitsvm (Xall, Yall, class_l, train_percentage, fold): Fits a poly kernel svm and returns the accuracy. Accepts parameter: train_percentage; fold; Returns: classifier, Accuracy.\n  best_combinations (class_l, train_percentage, fold): Finds all possible combination of classes and the accuracy for the given number of classes Accepts: Training percentage, and number of folds Returns: A List of best combination possible for given the class count.\n  getgenre (filename): Accepts a filename and returns a genre label for a given file.\n  getgenreMulti (filename): Accepts a filename and returns top three genre labels based on the probability.\n  acc Module for finding the accuracy.\n get ( res, test ) :\nCompares two arrays and returns the accuracy of their match.  Results    Classifier Training Accuracy Testing Accuracy     K-Nearest Neighbors  53%   Logistic Regression 75.778% 54%   SVM Linear Kernel 99% 52%   SVM RBF Kernel 99% 12%   SVM Poly Kernel 99% 64%    6 genre classes we are getting an accuracy of 85%\nConclusion We have tried various machine learning algorithms for this project. Our aim is to get maximum accuracy. We have found from our research that we can a get maximum accuracy of 65% by using poly kernel SVM for 10 genre classes. We have also tried to find the best combination of genre classes which will result in maximum accuracy. If we choose 6 genre classes we were able to get an accuracy of 85%. We chose these labels for the Web Application [classical, hip-hop, jazz, metal, pop and rock] For some songs we can say that it has feature of multiple genres. So we have also tried to get multiple label outputs based on the probability.\n","permalink":"https://indrajith.me/posts/machine-learning-pproach-to-classify-music-based-on-genre/","summary":"Music Genre Classifier Demo Music is categorized into subjective categories called genres. With the growth of the internet and multimedia, systems applications that deal with the musical databases gained importance and demand for Music Information Retrieval (MIR) applications increased.\nMusical genres have no strict definitions and boundaries as they arise through a complex interaction between the public, marketing, historical, and cultural factors. We are going to create an application that accepts a music file and classify it in to genres.","title":"Machine Learning Approach to Classify Music Based on Genre"},{"content":"Audio Visualizer For the given input audio file (.wav) the program will give the real time audio spectrum by performing Fast Fourier Transform (FFT) on the audio samples and plot the graph using OpenGL. This Project is implemented in C++ using OpenGL. Three frameworks Aquila-dsp, Kiss-Fft, and SFML are used to perform audio sampling, fast fourier transform and audio playback respectively. The development and testing of this project is done on Linux (ubuntu) using 4.2.0-16-generic Kernel. Interface for the program is given with the help of keyboard.\nFrameworks / APIs used  The following c++ frameworks are used in this project. 2 Simple and Fast Multimedia Library (SFML) Aquila-dsp Kiss-fft OpenGL  Minimum requirements: The development of the project was done with these requirements. The can support lower versions also. It has not been tested.\n C++ compiler : g++ with c++11 support (-std=gnu++11) (version \u0026gt;= 5.2.1) OpenGL (version \u0026gt;=v4.5) Aquila-dsp framework (version \u0026gt;=v3.0) Kiss-fft Framework (version \u0026gt;=v1.3.0) SFML framework \u0026gt;=2.3.2 Git \u0026gt;=v2.5.0  Keys Used: Use `Left/Right` to move horizontally. Use `Up/Down` to change the horizontal scale. Use `Home` button to reset the position and scale. Press ` F7 ` to toggle interpolation. Press `F8` to toggle clamping. Press `F9` to toggle drawing points. Press `q` to exit the program. Press `p` to to toggle audio play/pause. Press `r` to reload audio and play from the beginning. Press `right arrow` to seek audio forward by 5 seconds and move +ve x axis. Press `left arrow` to seek audio backward by 5 seconds and move -ve x axis. Press `down arrow` to decrease scale/1.5. Press `up arrow` to increase scale*1.5. Algorithm Step 1: Begin Int framePointer = 0, N = 32768. Step 2: Load input audio file and play it using the audio Library Step 3: For i = framePointer to → framePointer + N \u0026lt; total_samples_count Collect N samples from the audio file Step 4: Apply suitable window function (e.g. Hann aka Hanning window) Step 5: Apply Fast Fourier transform (FFT) on the array elements and collect N/2 Complex numbers having real part and imaginary part. (NB: if using typical complex-to-complex FFT then set imaginary parts of input array to zero) Step 6: Calculate the magnitude of N/2FFT data. magnitude = sqrt(re * re +img *img) Step 7: Convert magnitude to dB (log)scale. (optional) 20 * log10(magnitude) Step 8: Plot N/2 log(magnitude)values. Step 9: If N \u0026gt;= total_samples_count Exit. Else goto Step 3. Step 10: End. ## Flow Chart Compiling the frameworks Aquila-dsp: Prerequisites\nCMake: Aquila relies on CMake as a build tool, so make sure you have it installed (version 2.8 or later) Clone repository.\ngit clone git://github.com/zsiciarz/aquila.git aquila-src Buid\nWhere to build:\na-top-level-directory/ aquila-build/ aquila-src/ In directory you have to build do\ncmake ..\\aquila-src -DCMAKE_INSTALL_PREFIX=”~/home/mylib” make make install SFLM:\nInstalling SFML is simple. You need administrative privilege on the machine\nsudo apt-get install libsfml-dev Kiss FFT\nThere is no need to build kiss-fft. You need to add their files while compiling\nCompiling and running Clone the project from here: git clone git@github.com:indrajithi/Audio-Visualizer.git\nFor easy compilation use the script ./compile from the project directory\ng++ -std=c++11 -c draw.cpp g++ -std=gnu++11 draw.o \\ kiss_fft130/kiss_fft.c \\ -L /home/\u0026lt;YourUserName\u0026gt;/mylib/lib/ \\ -lAquila -lOoura_fft -lm \\ -lglut -lGLEW -lGL -lGLU \\ -lfreetype -lsfml-system \\ -lsfml-audio ./common/shader_utils.o \\ -o draw To run the project: ./draw \u0026lt;audio-input\u0026gt; Full Source Code.\n","permalink":"https://indrajith.me/posts/music-visualizer-in-c-using-opengl/","summary":"Audio Visualizer For the given input audio file (.wav) the program will give the real time audio spectrum by performing Fast Fourier Transform (FFT) on the audio samples and plot the graph using OpenGL. This Project is implemented in C++ using OpenGL. Three frameworks Aquila-dsp, Kiss-Fft, and SFML are used to perform audio sampling, fast fourier transform and audio playback respectively. The development and testing of this project is done on Linux (ubuntu) using 4.","title":"Music Visualizer in C Using OpenGL"},{"content":"Download the Dictionary First you have to download the dictionary words.txt https://github.com/indrajithi/anagram/blob/master/words.txt\nOpen the dictionary file in python Create a file called anagram.py and the add the following.\nfile = open(\u0026#39;words.txt\u0026#39;) wd_in = raw_input(\u0026#34;enter a jumbled word: \u0026#34;) found = 0 Function anagram Now lets define a function is_anagram()\ndef is_anagram(word1, word2): \u0026#34;\u0026#34;\u0026#34;Returns True if anagram is found. Else return False.\u0026#34;\u0026#34;\u0026#34; count = 0 word1, word2 = list(word1), list(word2) if len(word1) == len(word2): for i in word1: if i in word2: word2.remove(i) count += 1 if count == len(word1): return True return False Call the anagram function for line in file: if is_anagram(wd_in,line.strip()): found += 1 if found == 1: print \u0026#34;The possible anagrams are:\u0026#34; print line.strip() if found == 0: print \u0026#34;No possible match\u0026#34; else: print \u0026#34;Found %dmatch.\u0026#34; % found Complete code https://github.com/indrajithi/anagram\ndef is_anagram(word1, word2): \u0026#34;\u0026#34;\u0026#34;Returns True if anagram is found. Else return False.\u0026#34;\u0026#34;\u0026#34; count = 0 word1, word2 = list(word1), list(word2) if len(word1) == len(word2): for i in word1: if i in word2: word2.remove(i) count += 1 if count == len(word1): return True return False file = open(\u0026#39;words.txt\u0026#39;) wd_in = raw_input(\u0026#34;enter a jumbled word: \u0026#34;) found = 0 for line in file: if is_anagram(wd_in,line.strip()): found += 1 if found == 1: print \u0026#34;The possible anagrams are:\u0026#34; print line.strip() if found == 0: print \u0026#34;No possible mathch\u0026#34; else: print \u0026#34;Found %dmatch.\u0026#34; % found ","permalink":"https://indrajith.me/posts/jumble-words-solver-in-python/","summary":"Download the Dictionary First you have to download the dictionary words.txt https://github.com/indrajithi/anagram/blob/master/words.txt\nOpen the dictionary file in python Create a file called anagram.py and the add the following.\nfile = open(\u0026#39;words.txt\u0026#39;) wd_in = raw_input(\u0026#34;enter a jumbled word: \u0026#34;) found = 0 Function anagram Now lets define a function is_anagram()\ndef is_anagram(word1, word2): \u0026#34;\u0026#34;\u0026#34;Returns True if anagram is found. Else return False.\u0026#34;\u0026#34;\u0026#34; count = 0 word1, word2 = list(word1), list(word2) if len(word1) == len(word2): for i in word1: if i in word2: word2.","title":"Jumble Words solver using Python"},{"content":" Hugo PaperMod  ","permalink":"https://indrajith.me/credits/","summary":"credits","title":"Credits"}]